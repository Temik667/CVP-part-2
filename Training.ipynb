{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759481d2-6ae3-4b38-8064-a225de8ec741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:40.209109Z",
     "iopub.status.busy": "2024-12-24T11:39:40.208267Z",
     "iopub.status.idle": "2024-12-24T11:39:43.380003Z",
     "shell.execute_reply": "2024-12-24T11:39:43.379216Z",
     "shell.execute_reply.started": "2024-12-24T11:39:40.209081Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07883813-2311-4483-8870-d812a666abdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:43.382125Z",
     "iopub.status.busy": "2024-12-24T11:39:43.381336Z",
     "iopub.status.idle": "2024-12-24T11:39:43.384860Z",
     "shell.execute_reply": "2024-12-24T11:39:43.384183Z",
     "shell.execute_reply.started": "2024-12-24T11:39:43.382101Z"
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('/notebooks/tiny-imagenet.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/notebooks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f3bf6f-588a-423b-aba0-8741eee5f648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:49.358815Z",
     "iopub.status.busy": "2024-12-24T11:39:49.358004Z",
     "iopub.status.idle": "2024-12-24T11:39:49.366597Z",
     "shell.execute_reply": "2024-12-24T11:39:49.365826Z",
     "shell.execute_reply.started": "2024-12-24T11:39:49.358784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'batch_size': 64, 'name': 'resnet18_color_SGD_momentum', 'lr': 0.1, 'workers': 4, 'moment': 0.9, 'weight_decay': 0.0001, 'lr_step_size': 30, 'lr_gamma': 0.1, 'total_epochs': 500},\n",
       " 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.name = \"resnet18_color_SGD_momentum\"\n",
    "        self.lr = 0.1\n",
    "        self.workers = 4\n",
    "        self.moment = 0.9 ## Deleted bethas added moment\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_step_size = 30\n",
    "        self.lr_gamma = 0.1\n",
    "        self.total_epochs = 500\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "params = Params()\n",
    "params, params.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c4302b-4b09-4032-b899-09996813cce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:51.216655Z",
     "iopub.status.busy": "2024-12-24T11:39:51.215779Z",
     "iopub.status.idle": "2024-12-24T11:39:51.220252Z",
     "shell.execute_reply": "2024-12-24T11:39:51.219469Z",
     "shell.execute_reply.started": "2024-12-24T11:39:51.216631Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(image, label):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    plt.imshow(image.squeeze())\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ea7e73-38b8-46ee-9f19-5788715d14a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:52.309703Z",
     "iopub.status.busy": "2024-12-24T11:39:52.308763Z",
     "iopub.status.idle": "2024-12-24T11:39:52.316307Z",
     "shell.execute_reply": "2024-12-24T11:39:52.315636Z",
     "shell.execute_reply.started": "2024-12-24T11:39:52.309679Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "## to get paths and names of each image\n",
    "\n",
    "def images_get_paths(path):\n",
    "    paths = {}\n",
    "    names = {}\n",
    "    rel_paths = ''\n",
    "    for file in os.listdir(path):\n",
    "        if '.' not in file:\n",
    "            for img in os.listdir(path + '/' + file):\n",
    "                rel_paths = path + '/' + file + '/' + os.path.relpath(img)\n",
    "                if file not in paths.keys():\n",
    "                    paths[file] = [rel_paths]\n",
    "                    names[file] = [os.path.relpath(img)]\n",
    "                else:\n",
    "                    paths[file].append(rel_paths)\n",
    "                    names[file].append(os.path.relpath(img))\n",
    "    return paths, names\n",
    "\n",
    "## to organizes files from the given root path into a new structured directory.\n",
    "\n",
    "def organize_files(path):\n",
    "    new_root = os.path.join(\"Dataset\", os.path.basename(path) + \"_organized\")\n",
    "    os.makedirs(new_root, mode=0o777, exist_ok=True)\n",
    "    files_paths, files_names = images_get_paths(path)\n",
    "\n",
    "    for month in files_paths.keys():\n",
    "        month_folder = os.path.join(new_root, month)\n",
    "        os.makedirs(month_folder, mode=0o777, exist_ok=True)\n",
    "        \n",
    "        for name, file_path in zip(files_names[month], files_paths[month]):\n",
    "            category = name.split(\"_\")[0]\n",
    "            category_folder = os.path.join(month_folder, category)\n",
    "            os.makedirs(category_folder, mode=0o777, exist_ok=True)\n",
    "            shutil.move(file_path, os.path.join(category_folder, name))\n",
    "            print(f\"Moved: {file_path} -> {category_folder}\")\n",
    "# organize_files('Color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c845a00-76dc-4452-84ab-a81b107d648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:53.469082Z",
     "iopub.status.busy": "2024-12-24T11:39:53.468533Z",
     "iopub.status.idle": "2024-12-24T11:39:53.474120Z",
     "shell.execute_reply": "2024-12-24T11:39:53.473341Z",
     "shell.execute_reply.started": "2024-12-24T11:39:53.469055Z"
    }
   },
   "outputs": [],
   "source": [
    "## loader for different datasets\n",
    "\n",
    "## I deleted the convertion from BGR to RGB cause I redid dataset\n",
    "def Loader_train(root_folder):\n",
    "    train_transformation = transforms.Compose([\n",
    "            transforms.Resize((64,64)),\n",
    "            transforms.RandomVerticalFlip(0.5),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.RandomRotation(degrees=(180)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root = root_folder,\n",
    "        transform = train_transformation\n",
    "    )\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers = params.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc41225-eb30-4c48-a87d-a7dfaf71b613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:54.976804Z",
     "iopub.status.busy": "2024-12-24T11:39:54.976235Z",
     "iopub.status.idle": "2024-12-24T11:39:55.093079Z",
     "shell.execute_reply": "2024-12-24T11:39:55.092384Z",
     "shell.execute_reply.started": "2024-12-24T11:39:54.976779Z"
    }
   },
   "outputs": [],
   "source": [
    "## Validation loader\n",
    "val_transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        # transforms.CenterCrop(224),\n",
    "        transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "val_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='tiny-imagenet-200/val',\n",
    "    transform=val_transformation\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=params.workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1510cf1-cf92-46dd-8bfd-6eb25b63ec21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:56.180395Z",
     "iopub.status.busy": "2024-12-24T11:39:56.179832Z",
     "iopub.status.idle": "2024-12-24T11:39:56.186550Z",
     "shell.execute_reply": "2024-12-24T11:39:56.185856Z",
     "shell.execute_reply.started": "2024-12-24T11:39:56.180367Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    start = time.time()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = len(X)\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}], {(current/size * 100):>4f}%\")\n",
    "            step = epoch * size + current\n",
    "            writer.add_scalar('training loss', \n",
    "                            loss,\n",
    "                            step)\n",
    "            new_start = time.time()\n",
    "            delta = new_start - start\n",
    "            start = new_start\n",
    "            if batch != 0:\n",
    "                print(\"Done in \", delta, \" seconds\")\n",
    "                remaining_steps = size - current\n",
    "                speed = 100 * batch_size / delta\n",
    "                remaining_time = remaining_steps / speed\n",
    "                print(\"Remaining time (seconds): \", remaining_time)\n",
    "        optimizer.zero_grad()\n",
    "    print(\"Entire epoch done in \", time.time() - start0, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017ef53d-f806-485e-aadb-6977a8cdeb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:39:57.338763Z",
     "iopub.status.busy": "2024-12-24T11:39:57.338389Z",
     "iopub.status.idle": "2024-12-24T11:39:57.345824Z",
     "shell.execute_reply": "2024-12-24T11:39:57.344845Z",
     "shell.execute_reply.started": "2024-12-24T11:39:57.338739Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, epoch, writer, train_dataloader, calc_acc5=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, correct_top5 = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if calc_acc5:\n",
    "                _, pred_top5 = pred.topk(5, 1, largest=True, sorted=True)\n",
    "                correct_top5 += pred_top5.eq(y.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    step = epoch * len(train_dataloader.dataset)\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test loss',\n",
    "                            test_loss,\n",
    "                            step)\n",
    "    correct /= size\n",
    "    correct_top5 /= size\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test accuracy',\n",
    "                            100*correct,\n",
    "                            step)\n",
    "        if calc_acc5:\n",
    "            writer.add_scalar('test accuracy5',\n",
    "                            100*correct_top5,\n",
    "                            step)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if calc_acc5:\n",
    "        print(f\"Test Error: \\n Accuracy-5: {(100*correct_top5):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b389deb7-5b1e-409f-be96-ecb7ad7da3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:40:49.859315Z",
     "iopub.status.busy": "2024-12-24T11:40:49.858698Z",
     "iopub.status.idle": "2024-12-24T11:40:50.040693Z",
     "shell.execute_reply": "2024-12-24T11:40:50.039998Z",
     "shell.execute_reply.started": "2024-12-24T11:40:49.859286Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "model.fc=nn.Linear(model.fc.in_features, 200) ## Added this just to make sure that the output fits the labels\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "## Changed the optimizer her for SGD with momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = params.lr, momentum=params.moment, weight_decay = params.weight_decay)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params.lr_step_size, gamma=params.lr_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2fdb26e-9dfb-40a9-8d49-54ddffe724aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:40:50.650651Z",
     "iopub.status.busy": "2024-12-24T11:40:50.650021Z",
     "iopub.status.idle": "2024-12-24T11:40:55.273462Z",
     "shell.execute_reply": "2024-12-24T11:40:55.271605Z",
     "shell.execute_reply.started": "2024-12-24T11:40:50.650626Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "resume_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab8c4720-7b9e-4931-b93f-154297b1415c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:40:57.618662Z",
     "iopub.status.busy": "2024-12-24T11:40:57.618069Z",
     "iopub.status.idle": "2024-12-24T11:40:59.859349Z",
     "shell.execute_reply": "2024-12-24T11:40:59.858753Z",
     "shell.execute_reply.started": "2024-12-24T11:40:57.618635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-24 11:40:58.078654: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-24 11:40:58.078718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-24 11:40:58.080423: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-24 11:40:58.089398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-24 11:40:59.068145: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "start_dataset_idx = 1\n",
    "start_epoch = 1\n",
    "early_stopping_patience = 10\n",
    "no_improvement_count = 0\n",
    "best_val_accuracy = 0\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\")\n",
    "\n",
    "if resume_training and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    start_dataset_idx = checkpoint[\"dataset_idx\"]\n",
    "    best_val_accuracy = checkpoint.get(\"best_val_accuracy\", float('inf'))\n",
    "    no_improvement_count = checkpoint.get(\"no_improvement_count\", 0)\n",
    "    assert params == checkpoint[\"params\"]\n",
    "\n",
    "Path(os.path.join(\"checkpoints\", params.name)).mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter('runs/' + params.name)\n",
    "\n",
    "dataset_root = 'Dataset/Color_organized'\n",
    "dataset_folders = [os.path.join(dataset_root, f\"Color_{i}_months\") for i in range(0, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bda248c7-08ad-4c28-8029-ba0fcaf9161a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:41:01.700179Z",
     "iopub.status.busy": "2024-12-24T11:41:01.699222Z",
     "iopub.status.idle": "2024-12-24T11:41:01.703197Z",
     "shell.execute_reply": "2024-12-24T11:41:01.702738Z",
     "shell.execute_reply.started": "2024-12-24T11:41:01.700152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    }
   ],
   "source": [
    "print(start_epoch, start_dataset_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2314d419-e6f3-4d6e-b33b-0edcf53c6aae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:28:27.939542Z",
     "iopub.status.busy": "2024-12-24T11:28:27.939542Z",
     "iopub.status.idle": "2024-12-24T11:28:27.945720Z",
     "shell.execute_reply": "2024-12-24T11:28:27.943983Z",
     "shell.execute_reply.started": "2024-12-24T11:28:27.939542Z"
    }
   },
   "outputs": [],
   "source": [
    "# # for dataset_idx, dataset_folder in enumerate(dataset_folders, start=1):\n",
    "# #     if dataset_idx < start_dataset_idx:\n",
    "# #         continue\n",
    "\n",
    "# #     print(f\"Training on dataset {dataset_idx} at {dataset_folder}\")\n",
    "# train_loader = Loader_train(dataset_root)\n",
    "\n",
    "# for epoch in range(start_epoch, params.total_epochs):\n",
    "#     train(train_loader, model, loss_fn, optimizer, epoch, writer)\n",
    "\n",
    "#     val_accuracy = test(val_loader, model, loss_fn, epoch, writer, train_dataloader=train_loader, calc_acc5=True)\n",
    "\n",
    "#     if val_accuracy > best_val_accuracy:\n",
    "#         best_val_accuracy = val_accuracy\n",
    "#         no_improvement_count = 0\n",
    "#         checkpoint = {\n",
    "#             \"model\": model.state_dict(),\n",
    "#             \"optimizer\": optimizer.state_dict(),\n",
    "#             \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "#             \"epoch\": epoch,\n",
    "#             # \"dataset_idx\": dataset_idx,\n",
    "#             \"params\": params,\n",
    "#             \"best_val_accuracy\": best_val_accuracy,\n",
    "#             \"no_improvement_count\": no_improvement_count,\n",
    "#             }\n",
    "#         torch.save(checkpoint, checkpoint_path)\n",
    "#         print(f\"Checkpoint successfully saved at {checkpoint_path}\")\n",
    "\n",
    "#         print(f\"New best validation accuracy: {val_accuracy:.4f}\")\n",
    "#     else:\n",
    "#         no_improvement_count += 1\n",
    "#         print(f\"No improvement for {no_improvement_count} epochs.\")\n",
    "\n",
    "#     if no_improvement_count >= early_stopping_patience:\n",
    "#         print(f\"Early stopping triggered after {epoch} epochs with no improvement.\")\n",
    "#         break\n",
    "\n",
    "# best_val_accuracy = 0\n",
    "# start_epoch = 1  \n",
    "# no_improvement_count = 0\n",
    "# print(f\"Finished training on dataset {dataset_idx}.\")\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab7983-f008-4cc3-b94b-459d708514ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-24T11:41:23.956331Z",
     "iopub.status.busy": "2024-12-24T11:41:23.956017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset 1 at Dataset/Color_organized/Color_0_months\n",
      "loss: 5.475552  [   64/100000], 0.064000%\n",
      "loss: 5.317505  [ 6464/100000], 6.464000%\n",
      "Done in  9.130945444107056  seconds\n",
      "Remaining time (seconds):  133.44876766562462\n",
      "loss: 5.291832  [12864/100000], 12.864000%\n",
      "Done in  10.999911546707153  seconds\n",
      "Remaining time (seconds):  149.7637957084179\n",
      "loss: 5.213200  [19264/100000], 19.264000%\n",
      "Done in  10.457078218460083  seconds\n",
      "Remaining time (seconds):  131.91604172587395\n",
      "loss: 5.180685  [25664/100000], 25.664000%\n",
      "Done in  11.054276704788208  seconds\n",
      "Remaining time (seconds):  128.39542392611503\n",
      "loss: 5.110563  [32064/100000], 32.064000%\n",
      "Done in  11.084460735321045  seconds\n",
      "Remaining time (seconds):  117.66155070543289\n",
      "loss: 5.076426  [38464/100000], 38.464000%\n",
      "Done in  11.73805284500122  seconds\n",
      "Remaining time (seconds):  112.86137810468675\n",
      "loss: 4.858923  [44864/100000], 44.864000%\n",
      "Done in  12.07607126235962  seconds\n",
      "Remaining time (seconds):  104.03535392522812\n",
      "loss: 5.137281  [51264/100000], 51.264000%\n",
      "Done in  13.691237211227417  seconds\n",
      "Remaining time (seconds):  104.25877136349678\n",
      "loss: 4.906940  [57664/100000], 57.664000%\n",
      "Done in  12.433717489242554  seconds\n",
      "Remaining time (seconds):  82.24904119133949\n"
     ]
    }
   ],
   "source": [
    "for dataset_idx, dataset_folder in enumerate(dataset_folders, start=1):\n",
    "    if dataset_idx < start_dataset_idx:\n",
    "        continue\n",
    "\n",
    "    print(f\"Training on dataset {dataset_idx} at {dataset_folder}\")\n",
    "    train_loader = Loader_train(dataset_folder)\n",
    "\n",
    "    for epoch in range(start_epoch if dataset_idx == start_dataset_idx else 1, params.total_epochs):\n",
    "        train(train_loader, model, loss_fn, optimizer, epoch, writer)\n",
    "        \n",
    "        val_accuracy = test(val_loader, model, loss_fn, epoch, writer, train_dataloader=train_loader, calc_acc5=True)\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            no_improvement_count = 0\n",
    "            \n",
    "            checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"dataset_idx\": dataset_idx,\n",
    "            \"params\": params,\n",
    "            \"best_val_accuracy\": best_val_accuracy,\n",
    "            \"no_improvement_count\": no_improvement_count,\n",
    "            }\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            print(f\"Checkpoint successfully saved at {checkpoint_path}\")\n",
    "            print(f\"New best validation accuracy: {val_accuracy:.4f}\")\n",
    "            \n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "            print(f\"No improvement for {no_improvement_count} epochs.\")\n",
    "\n",
    "        if no_improvement_count >= early_stopping_patience:\n",
    "            print(f\"Early stopping triggered after {epoch} epochs with no improvement.\")\n",
    "            break\n",
    "    \n",
    "    \n",
    "    best_val_accuracy = 0\n",
    "    start_epoch = 1  \n",
    "    no_improvement_count = 0\n",
    "    print(f\"Finished training on dataset {dataset_idx}.\")\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e401498-8c52-4144-98e4-ba45b0b74ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-22T12:03:28.977615Z",
     "iopub.status.busy": "2024-12-22T12:03:28.976498Z",
     "iopub.status.idle": "2024-12-22T12:03:40.410793Z",
     "shell.execute_reply": "2024-12-22T12:03:40.409824Z",
     "shell.execute_reply.started": "2024-12-22T12:03:28.977573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 0.3%, Avg loss: 21.305120 \n",
      "\n",
      "Test Error: \n",
      " Accuracy-5: 0.7%, Avg loss: 21.305120 \n",
      "\n",
      "0.002803976548559776\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = test(val_loader, model, loss_fn, epoch, writer, train_dataloader=train_loader, calc_acc5=True)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a54bb-20a0-451a-adc8-4df14f828e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standard Training\n",
    "# for epoch in range(start_epoch, 100):\n",
    "#     train(train_loader, model, loss_fn, optimizer, epoch=epoch, writer=writer)\n",
    "#     checkpoint = {\n",
    "#         \"model\": model.state_dict(),\n",
    "#         \"optimizer\": optimizer.state_dict(),\n",
    "#         \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "#         \"epoch\": epoch,\n",
    "#         \"params\": params\n",
    "#     }\n",
    "#     torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"model_{epoch}.pth\"))\n",
    "#     torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\"))\n",
    "#     lr_scheduler.step()\n",
    "#     test(val_loader, model, loss_fn, epoch + 1, writer, train_dataloader=train_loader, calc_acc5=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
