{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759481d2-6ae3-4b38-8064-a225de8ec741",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:10:49.555984Z",
     "iopub.status.busy": "2024-12-17T08:10:49.555468Z",
     "iopub.status.idle": "2024-12-17T08:10:55.651448Z",
     "shell.execute_reply": "2024-12-17T08:10:55.650591Z",
     "shell.execute_reply.started": "2024-12-17T08:10:49.555959Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc684562-6c2b-404e-8353-5d51b9442330",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-16T14:51:52.299036Z",
     "iopub.status.busy": "2024-12-16T14:51:52.298698Z",
     "iopub.status.idle": "2024-12-16T14:53:34.285338Z",
     "shell.execute_reply": "2024-12-16T14:53:34.284398Z",
     "shell.execute_reply.started": "2024-12-16T14:51:52.299011Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1zNFXjlsSFuUlfsB6lHd_ybe224qmHCUW\n",
      "From (redirected): https://drive.google.com/uc?id=1zNFXjlsSFuUlfsB6lHd_ybe224qmHCUW&confirm=t&uuid=4b9d7382-0e69-488c-b38e-43d3a3581e8b\n",
      "To: /Color.zip\n",
      "100%|██████████| 9.31G/9.31G [01:37<00:00, 95.1MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Color.zip'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gdown\n",
    "\n",
    "# url = 'https://drive.google.com/file/d/1zNFXjlsSFuUlfsB6lHd_ybe224qmHCUW/view?usp=sharing'\n",
    "# output_path = '/'\n",
    "# gdown.download(url, output_path, quiet=False,fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07883813-2311-4483-8870-d812a666abdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T15:26:24.663913Z",
     "iopub.status.busy": "2024-12-16T15:26:24.663516Z",
     "iopub.status.idle": "2024-12-16T15:34:07.513901Z",
     "shell.execute_reply": "2024-12-16T15:34:07.511842Z",
     "shell.execute_reply.started": "2024-12-16T15:26:24.663879Z"
    }
   },
   "outputs": [],
   "source": [
    "# import zipfile\n",
    "# with zipfile.ZipFile('/notebooks/Color.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('/notebooks/Transformation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02f3bf6f-588a-423b-aba0-8741eee5f648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:10:59.932692Z",
     "iopub.status.busy": "2024-12-17T08:10:59.932205Z",
     "iopub.status.idle": "2024-12-17T08:11:00.006324Z",
     "shell.execute_reply": "2024-12-17T08:11:00.005374Z",
     "shell.execute_reply.started": "2024-12-17T08:10:59.932667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'batch_size': 32, 'name': 'resnet_34_adam', 'lr': 0.1, 'workers': 4, 'betha_1': 0.9, 'betha_2': 0.999, 'epsilon': 1e-06, 'weight_decay': 0.0001, 'lr_step_size': 30, 'lr_gamma': 0.1},\n",
       " 32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Params:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 32\n",
    "        self.name = \"resnet_34_adam\"\n",
    "        self.lr = 0.1\n",
    "        self.workers = 4\n",
    "        self.betha_1 = 0.9\n",
    "        self.betha_2 = 0.999\n",
    "        self.epsilon = 10e-7\n",
    "        self.weight_decay = 1e-4\n",
    "        self.lr_step_size = 30\n",
    "        self.lr_gamma = 0.1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.__dict__ == other.__dict__\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "params = Params()\n",
    "params, params.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54c4302b-4b09-4032-b899-09996813cce4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:02.700218Z",
     "iopub.status.busy": "2024-12-17T08:11:02.699452Z",
     "iopub.status.idle": "2024-12-17T08:11:02.705883Z",
     "shell.execute_reply": "2024-12-17T08:11:02.704936Z",
     "shell.execute_reply.started": "2024-12-17T08:11:02.700191Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_image(image, label):\n",
    "    image = image.permute(1, 2, 0)\n",
    "    plt.imshow(image.squeeze())\n",
    "    plt.title(f'Label: {label}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ea7e73-38b8-46ee-9f19-5788715d14a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:05.285334Z",
     "iopub.status.busy": "2024-12-17T08:11:05.285016Z",
     "iopub.status.idle": "2024-12-17T08:11:05.292688Z",
     "shell.execute_reply": "2024-12-17T08:11:05.291965Z",
     "shell.execute_reply.started": "2024-12-17T08:11:05.285310Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "## to get paths and names of each image\n",
    "\n",
    "def images_get_paths(path):\n",
    "    paths = {}\n",
    "    names = {}\n",
    "    rel_paths = ''\n",
    "    for file in os.listdir(path):\n",
    "        if '.' not in file:\n",
    "            for img in os.listdir(path + '/' + file):\n",
    "                rel_paths = path + '/' + file + '/' + os.path.relpath(img)\n",
    "                if file not in paths.keys():\n",
    "                    paths[file] = [rel_paths]\n",
    "                    names[file] = [os.path.relpath(img)]\n",
    "                else:\n",
    "                    paths[file].append(rel_paths)\n",
    "                    names[file].append(os.path.relpath(img))\n",
    "    return paths, names\n",
    "\n",
    "## to organizes files from the given root path into a new structured directory.\n",
    "\n",
    "def organize_files(path):\n",
    "    new_root = os.path.join(\"Dataset\", os.path.basename(path) + \"_organized\")\n",
    "    os.makedirs(new_root, mode=0o777, exist_ok=True)\n",
    "    files_paths, files_names = images_get_paths(path)\n",
    "\n",
    "    for month in files_paths.keys():\n",
    "        month_folder = os.path.join(new_root, month)\n",
    "        os.makedirs(month_folder, mode=0o777, exist_ok=True)\n",
    "        \n",
    "        for name, file_path in zip(files_names[month], files_paths[month]):\n",
    "            category = name.split(\"_\")[0]\n",
    "            category_folder = os.path.join(month_folder, category)\n",
    "            os.makedirs(category_folder, mode=0o777, exist_ok=True)\n",
    "            shutil.move(file_path, os.path.join(category_folder, name))\n",
    "            print(f\"Moved: {file_path} -> {category_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c845a00-76dc-4452-84ab-a81b107d648b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:07.376556Z",
     "iopub.status.busy": "2024-12-17T08:11:07.375640Z",
     "iopub.status.idle": "2024-12-17T08:11:07.383414Z",
     "shell.execute_reply": "2024-12-17T08:11:07.382330Z",
     "shell.execute_reply.started": "2024-12-17T08:11:07.376519Z"
    }
   },
   "outputs": [],
   "source": [
    "## loader for different datasets\n",
    "def Loader_train(root_folder):\n",
    "    train_transformation = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.RandomResizedCrop(224, interpolation=transforms.InterpolationMode.BILINEAR, antialias=True),\n",
    "            transforms.RandomHorizontalFlip(0.5),\n",
    "            transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root = root_folder,\n",
    "        transform = train_transformation\n",
    "    )\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=params.batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers = params.workers,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adc41225-eb30-4c48-a87d-a7dfaf71b613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:09.839509Z",
     "iopub.status.busy": "2024-12-17T08:11:09.839158Z",
     "iopub.status.idle": "2024-12-17T08:11:10.246446Z",
     "shell.execute_reply": "2024-12-17T08:11:10.245735Z",
     "shell.execute_reply.started": "2024-12-17T08:11:09.839482Z"
    }
   },
   "outputs": [],
   "source": [
    "## Validation loader\n",
    "val_transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(size=256, antialias=True),\n",
    "        transforms.CenterCrop(224),\n",
    "        # Normalize the pixel values (in R, G, and B channels)\n",
    "        transforms.Normalize(mean=[0.485, 0.485, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "val_dataset = torchvision.datasets.ImageFolder(\n",
    "    root='imagenet-mini/val',\n",
    "    transform=val_transformation\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=params.workers,\n",
    "    shuffle=False,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1510cf1-cf92-46dd-8bfd-6eb25b63ec21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:14.341180Z",
     "iopub.status.busy": "2024-12-17T08:11:14.340439Z",
     "iopub.status.idle": "2024-12-17T08:11:14.347659Z",
     "shell.execute_reply": "2024-12-17T08:11:14.346868Z",
     "shell.execute_reply.started": "2024-12-17T08:11:14.341155Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def train(dataloader, model, loss_fn, optimizer, epoch, writer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    start0 = time.time()\n",
    "    start = time.time()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_size = len(X)\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}], {(current/size * 100):>4f}%\")\n",
    "            step = epoch * size + current\n",
    "            writer.add_scalar('training loss',\n",
    "                            loss,\n",
    "                            step)\n",
    "            new_start = time.time()\n",
    "            delta = new_start - start\n",
    "            start = new_start\n",
    "            if batch != 0:\n",
    "                print(\"Done in \", delta, \" seconds\")\n",
    "                remaining_steps = size - current\n",
    "                speed = 100 * batch_size / delta\n",
    "                remaining_time = remaining_steps / speed\n",
    "                print(\"Remaining time (seconds): \", remaining_time)\n",
    "        optimizer.zero_grad()\n",
    "    print(\"Entire epoch done in \", time.time() - start0, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017ef53d-f806-485e-aadb-6977a8cdeb63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:16.653667Z",
     "iopub.status.busy": "2024-12-17T08:11:16.653328Z",
     "iopub.status.idle": "2024-12-17T08:11:16.660590Z",
     "shell.execute_reply": "2024-12-17T08:11:16.659812Z",
     "shell.execute_reply.started": "2024-12-17T08:11:16.653641Z"
    }
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, epoch, writer, train_dataloader, calc_acc5=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, correct_top5 = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            if calc_acc5:\n",
    "                _, pred_top5 = pred.topk(5, 1, largest=True, sorted=True)\n",
    "                correct_top5 += pred_top5.eq(y.view(-1, 1).expand_as(pred_top5)).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    step = epoch * len(train_dataloader.dataset)\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test loss',\n",
    "                            test_loss,\n",
    "                            step)\n",
    "    correct /= size\n",
    "    correct_top5 /= size\n",
    "    if writer != None:\n",
    "        writer.add_scalar('test accuracy',\n",
    "                            100*correct,\n",
    "                            step)\n",
    "        if calc_acc5:\n",
    "            writer.add_scalar('test accuracy5',\n",
    "                            100*correct_top5,\n",
    "                            step)\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    if calc_acc5:\n",
    "        print(f\"Test Error: \\n Accuracy-5: {(100*correct_top5):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b389deb7-5b1e-409f-be96-ecb7ad7da3dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:18.795098Z",
     "iopub.status.busy": "2024-12-17T08:11:18.794664Z",
     "iopub.status.idle": "2024-12-17T08:11:19.200861Z",
     "shell.execute_reply": "2024-12-17T08:11:19.200057Z",
     "shell.execute_reply.started": "2024-12-17T08:11:18.795075Z"
    }
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet34()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = params.lr, betas = (params.betha_1, params.betha_2),\n",
    "                            eps = params.epsilon, weight_decay = params.weight_decay)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params.lr_step_size, gamma=params.lr_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2fdb26e-9dfb-40a9-8d49-54ddffe724aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:20.727043Z",
     "iopub.status.busy": "2024-12-17T08:11:20.726734Z",
     "iopub.status.idle": "2024-12-17T08:11:26.201563Z",
     "shell.execute_reply": "2024-12-17T08:11:26.200780Z",
     "shell.execute_reply.started": "2024-12-17T08:11:20.727019Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "resume_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8c4720-7b9e-4931-b93f-154297b1415c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:37.256910Z",
     "iopub.status.busy": "2024-12-17T08:11:37.256017Z",
     "iopub.status.idle": "2024-12-17T08:11:37.267364Z",
     "shell.execute_reply": "2024-12-17T08:11:37.266563Z",
     "shell.execute_reply.started": "2024-12-17T08:11:37.256882Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "\n",
    "start_dataset_idx = 1\n",
    "start_epoch = 1\n",
    "\n",
    "checkpoint_path = os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\")\n",
    "if resume_training and os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    start_dataset_idx = checkpoint[\"dataset_idx\"]\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    lr_scheduler.load_state_dict(checkpoint[\"lr_scheduler\"])\n",
    "    assert params == checkpoint[\"params\"]\n",
    "\n",
    "Path(os.path.join(\"checkpoints\", params.name)).mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter('runs/' + params.name)\n",
    "\n",
    "dataset_root = 'Color'\n",
    "dataset_folders = [os.path.join(dataset_root, f\"color_{i}_months\") for i in range(0, 13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51ab7983-f008-4cc3-b94b-459d708514ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T08:11:43.131086Z",
     "iopub.status.busy": "2024-12-17T08:11:43.130369Z",
     "iopub.status.idle": "2024-12-17T08:12:12.643850Z",
     "shell.execute_reply": "2024-12-17T08:12:12.642643Z",
     "shell.execute_reply.started": "2024-12-17T08:11:43.131061Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on dataset 1 at Color/color_0_months\n",
      "loss: 7.141317  [   32/34745], 0.092100%\n",
      "loss: 7.044847  [ 3232/34745], 9.302058%\n",
      "Done in  8.91267442703247  seconds\n",
      "Remaining time (seconds):  87.7703466309607\n",
      "loss: 6.937400  [ 6432/34745], 18.512016%\n",
      "Done in  8.92616605758667  seconds\n",
      "Remaining time (seconds):  78.97704362139106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m Loader_train(dataset_folder)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch \u001b[38;5;28;01mif\u001b[39;00m dataset_idx \u001b[38;5;241m==\u001b[39m start_dataset_idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: params,\n\u001b[1;32m     17\u001b[0m     }\n\u001b[1;32m     18\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(checkpoint, checkpoint_path)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, epoch, writer)\u001b[0m\n\u001b[1;32m      6\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 8\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_idx, dataset_folder in enumerate(dataset_folders, start=1):\n",
    "    if dataset_idx < start_dataset_idx:\n",
    "        continue\n",
    "\n",
    "    print(f\"Training on dataset {dataset_idx} at {dataset_folder}\")\n",
    "    train_loader = Loader_train(dataset_folder)\n",
    "    \n",
    "    for epoch in range(start_epoch if dataset_idx == start_dataset_idx else 1, 2):\n",
    "        train(train_loader, model, loss_fn, optimizer, epoch, writer)\n",
    "        checkpoint = {\n",
    "            \"model\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "            \"dataset_idx\": dataset_idx,\n",
    "            \"params\": params,\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint successfully saved at {checkpoint_path}\")\n",
    "        test(val_loader, model, loss_fn, epoch, writer, train_dataloader=train_loader, calc_acc5=True)\n",
    "\n",
    "    start_epoch = 1\n",
    "    print(f\"Finished training on dataset {dataset_idx}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a54bb-20a0-451a-adc8-4df14f828e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Standard Training\n",
    "for epoch in range(start_epoch, 100):\n",
    "    train(train_loader, model, loss_fn, optimizer, epoch=epoch, writer=writer)\n",
    "    checkpoint = {\n",
    "        \"model\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "        \"epoch\": epoch,\n",
    "        \"params\": params\n",
    "    }\n",
    "    torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"model_{epoch}.pth\"))\n",
    "    torch.save(checkpoint, os.path.join(\"checkpoints\", params.name, f\"checkpoint.pth\"))\n",
    "    lr_scheduler.step()\n",
    "    test(val_loader, model, loss_fn, epoch + 1, writer, train_dataloader=train_loader, calc_acc5=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
